{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf782def-ea3c-45ad-ae9e-610f287a5bba",
   "metadata": {},
   "source": [
    "#### **This is the first version of PageRank on MapReduce.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a1cb0-3bb9-45a9-b0ac-72b3254ec0b7",
   "metadata": {},
   "source": [
    "The Internet is stored as a big matrix ***M*** (size n × n). Specifically the column-normalized adjacency matrix\n",
    "where each column represents a webpage and where it links to are the non-zero entries.\n",
    "Break M into k vertical stripes M = [M1 M2 . . . Mk] so each Mj fits on a machine. İnitiate ***q*** as a vector of PageRank with  values as 1/number of pages(n). \n",
    "* ***Mapper:***  j → key= j' ∈ [k] ; value = row r of Mj ∗ qj \n",
    "* ***Reducer:***  adds values for each key i to get qi+1[j] ∗ β + (1 − β)/n.\n",
    "\n",
    "typically β = 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d222213a-3c70-489a-835d-e7c2339bc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f496991b-4c09-44a1-b751-64246da1fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"G1.pkl\", \"rb\") as file:\n",
    "    G1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7befc54-7236-4350-9879-cd2efc80200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_G1(data,N=G1.number_of_nodes()):\n",
    "    top=data['hero'].value_counts().head(N)\n",
    "    hero_list=top.index.tolist()\n",
    "    return hero_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16fc95b-eb99-4d1d-80c6-6877883c633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe with the edges datas\n",
    "df_edges = pd.read_csv('edges.csv')\n",
    "\n",
    "#Extract the subgraph of the top_N heros for the first graph\n",
    "hero_list=top_N_G1(df_edges, 500)\n",
    "top=G1.subgraph(hero_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79cd14c-1604-4922-badb-41e1287f6ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create the adjacency matrix of the first graph\n",
    "M_G1=nx.to_numpy_matrix(top)\n",
    "Matrix=M_G1.tolist()\n",
    "\n",
    "#Save the length of M_G1 as n\n",
    "n1=len(M_G1)\n",
    "\n",
    "#Beta is usually sets as 0.85\n",
    "beta=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17a52e7a-b167-4eac-be67-a73180bafb25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set up the Spark context\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9fd14c-49b7-4c6f-8385-8fc6eb2a99a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Convert M_G1 to RDDs (into k vertical stripes)\n",
    "M_rdd = sc.parallelize(np.transpose(Matrix))\n",
    "rddCollect = M_rdd.collect()\n",
    "N=M_rdd.count()\n",
    "\n",
    "#Inizialize q \n",
    "q =[]\n",
    "for i in range(N):\n",
    "    q.append(1/N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d533dbd-31c7-49b8-8245-257207136bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(row,q):\n",
    "    m=[]\n",
    "    for i in range(len(row)):\n",
    "        m.append(((i+1),row[i]*q[0]))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152fd5c0-e113-492e-b8e6-92f048a27b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.10016190035063106),\n",
       " (4, 0.1986695519750855),\n",
       " (6, 0.17743980344805738),\n",
       " (8, 0.08122033085994863),\n",
       " (10, 0.1233882818284333),\n",
       " (12, 0.11635272965760857),\n",
       " (14, 0.6187876090383257),\n",
       " (16, 0.2654296840031845),\n",
       " (18, 0.2086029757399857),\n",
       " (20, 0.07413488952437754)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map the grouped tiles\n",
    "M_mapped=M_rdd.flatMap(lambda x: mapper(x,q))\n",
    "\n",
    "#Reduce the mapped output by adding the values for each row\n",
    "M_reduced = M_mapped.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "#Update the values of q using the map-reduce output\n",
    "q_updated = M_reduced.map(lambda row: (row[0], row[1] * beta + (1 - beta) / n1))\n",
    "\n",
    "#Return the updated values of q\n",
    "c=q_updated.collect()\n",
    "c[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6ad91f-afa4-464a-aabb-8852fb92d540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Take matrix M from the example on the pdf file\n",
    "M=[[0, 0.5, 0, 0],\n",
    "   [0.3, 0, 1, 0.5],\n",
    "   [0.3, 0, 0, 0.5],\n",
    "   [0.3, 0.5, 0, 0]]\n",
    "\n",
    "n=len(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a03b4d3-5d29-446d-9766-de8748d70f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert M to RDDs (into k vertical stripes)\n",
    "M_rdd = sc.parallelize(np.transpose(M))\n",
    "rddCollect = M_rdd.collect()\n",
    "N=M_rdd.count()\n",
    "\n",
    "#Inizialize q \n",
    "q =[]\n",
    "for i in range(N):\n",
    "    q.append(1/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1789cd1-a3df-4515-8c46-8aeb4e72f795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.42000000000000004),\n",
       " (4, 0.20750000000000002),\n",
       " (1, 0.14375),\n",
       " (3, 0.20750000000000002)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map the grouped tiles\n",
    "M_mapped=M_rdd.flatMap(lambda x: mapper(x,q))\n",
    "\n",
    "#Reduce the mapped output by adding the values for each row\n",
    "M_reduced = M_mapped.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "#Update the values of q using the map-reduce output\n",
    "q_updated = M_reduced.map(lambda row: (row[0], row[1] * beta + (1 - beta) / n))\n",
    "\n",
    "#Return the updated values of q\n",
    "q_updated.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050f6c8-42c8-4c07-a195-ac2f0097e2e8",
   "metadata": {},
   "source": [
    "Where q is a probability vector with tuples where the first values are equal to the number of a spacific node (one of n), the second values are the probability that you are in that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf7ca6c-49a8-4cc3-a7c7-05dff66b6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the Spark context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

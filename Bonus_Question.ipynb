{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf782def-ea3c-45ad-ae9e-610f287a5bba",
   "metadata": {},
   "source": [
    "#### **This is the first version of PageRank on MapReduce.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a1cb0-3bb9-45a9-b0ac-72b3254ec0b7",
   "metadata": {},
   "source": [
    "The Internet is stored as a big matrix ***M*** (size n × n). Specifically the column-normalized adjacency matrix\n",
    "where each column represents a webpage and where it links to are the non-zero entries.\n",
    "Break M into k vertical stripes M = [M1 M2 . . . Mk] so each Mj fits on a machine. İnitiate ***q*** as a vector of PageRank with  values as 1/number of pages(n). \n",
    "* ***Mapper:***  j → key= j' ∈ [k] ; value = row r of Mj ∗ qj \n",
    "* ***Reducer:***  adds values for each key i to get qi+1[j] ∗ β + (1 − β)/n.\n",
    "\n",
    "typically β = 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d222213a-3c70-489a-835d-e7c2339bc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a52e7a-b167-4eac-be67-a73180bafb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/03 09:52:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Set up the Spark context\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6ad91f-afa4-464a-aabb-8852fb92d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take matrix M from the example on the pdf file\n",
    "M=[[0, 0.5, 0, 0],\n",
    "   [0.3, 0, 1, 0.5],\n",
    "   [0.3, 0, 0, 0.5],\n",
    "   [0.3, 0.5, 0, 0]]\n",
    "\n",
    "n=len(M)\n",
    "\n",
    "#Beta is usually sets as 0.85\n",
    "beta=0.85\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9fd14c-49b7-4c6f-8385-8fc6eb2a99a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0. , 0.3, 0.3, 0.3]), array([0.5, 0. , 0. , 0.5]), array([0., 1., 0., 0.]), array([0. , 0.5, 0.5, 0. ])]\n",
      "[0.25, 0.25, 0.25, 0.25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Convert M to RDDs (into k vertical stripes)\n",
    "M_rdd = sc.parallelize(np.transpose(M))\n",
    "rddCollect = M_rdd.collect()\n",
    "N=M_rdd.count()\n",
    "\n",
    "#Inizialize q \n",
    "q =[]\n",
    "for i in range(N):\n",
    "    q.append(1/N)\n",
    "\n",
    "\n",
    "print(M_rdd.collect())\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d533dbd-31c7-49b8-8245-257207136bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(row,q):\n",
    "    m=[]\n",
    "    for i in range(len(row)):\n",
    "        m.append(((i+1),row[i]*q[0]))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152fd5c0-e113-492e-b8e6-92f048a27b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.42000000000000004),\n",
       " (4, 0.20750000000000002),\n",
       " (1, 0.14375),\n",
       " (3, 0.20750000000000002)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map the grouped tiles\n",
    "M_mapped=M_rdd.flatMap(lambda x: mapper(x,q))\n",
    "\n",
    "#Reduce the mapped output by adding the values for each row\n",
    "M_reduced = M_mapped.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "#Update the values of q using the map-reduce output\n",
    "q_updated = M_reduced.map(lambda row: (row[0], row[1] * beta + (1 - beta) / n))\n",
    "\n",
    "#Return the updated values of q\n",
    "q_updated.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050f6c8-42c8-4c07-a195-ac2f0097e2e8",
   "metadata": {},
   "source": [
    "Where q is a probability vector with tuples where the first values are equal to the number of a spacific node (one of n), the second values are the probability that you are in that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf7ca6c-49a8-4cc3-a7c7-05dff66b6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the Spark context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
